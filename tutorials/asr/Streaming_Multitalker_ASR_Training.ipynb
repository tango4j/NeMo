{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJz6FDU1lRzc"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell.\n",
    "\n",
    "## Install dependencies\n",
    "!pip install wget\n",
    "!apt-get install sox libsndfile1 ffmpeg\n",
    "!pip install text-unidecode\n",
    "!pip install ipython\n",
    "\n",
    "# ## Install NeMo\n",
    "BRANCH = 'main'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@{BRANCH}#egg=nemo_toolkit[asr]\n",
    "\n",
    "## Install TorchAudio\n",
    "!pip install torchaudio -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Multitalker ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1Jk9etFlRzf"
   },
   "source": [
    "## Streaming Multitalker ASR with Self-Speaker Adaptation\n",
    "\n",
    "This tutorial shows you how to use NeMo's streaming multitalker ASR system based on the approach described in [(Wang et al., 2025)](https://arxiv.org/abs/2506.22646). This system transcribes each speaker separately in multispeaker audio using speaker activity information from a streaming diarization model.\n",
    "\n",
    "### How This Approach Works\n",
    "\n",
    "The streaming multitalker Parakeet model uses **self-speaker adaptation (SSA)**, which means:\n",
    "\n",
    "1. **No Speaker Enrollment Required**: You only need speaker activity predictions from a diarization model (like Streaming Sortformer)\n",
    "2. **Speaker Kernel Injection**: The model injects speaker-specific kernels into encoder layers to focus on each target speaker\n",
    "3. **Multi-Instance Architecture**: You run one model instance per speaker, and each instance processes the same audio\n",
    "4. **Handles Overlapping Speech**: Each instance focuses on one speaker, so it can transcribe overlapped speech segments\n",
    "\n",
    "### Cache-Aware Streaming\n",
    "\n",
    "The model uses stateful cache-based inference [(Noroozi et al., 2023)](https://arxiv.org/abs/2312.17279) for streaming:\n",
    "- Left and right contexts in the encoder are constrained for low latency\n",
    "- An activation caching mechanism enables the encoder to operate autoregressively during inference\n",
    "- The model maintains consistent behavior between training and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: Multi-Instance Architecture Overview\n",
    "\n",
    "The streaming multitalker Parakeet model employs a **multi-instance approach** where one model instance is deployed per speaker:\n",
    "\n",
    "<img src=\"images/multi_instance.png\" alt=\"Multi-instance inference of Multitalker Parakeet model\" style=\"width: 800px;\"/>\n",
    "\n",
    "Each model instance:\n",
    "- Receives the same mixed audio input\n",
    "- Injects **speaker-specific kernels** generated from diarization-based speaker activity\n",
    "- Produces transcription output specific to its target speaker\n",
    "- Operates independently and can run in parallel with other instances\n",
    "\n",
    "### Speaker Kernel Injection Mechanism\n",
    "\n",
    "Learnable speaker kernels are injected into selected layers of the Fast-Conformer encoder:\n",
    "\n",
    "<img src=\"images/speaker_injection.png\" alt=\"Speaker Kernel Injection Mechanism\" style=\"width: 800px;\"/>\n",
    "\n",
    "The speaker kernels are generated through speaker supervision activations that detect speech activity for each target speaker from the streaming diarization output. This enables the encoder states to become more responsive to the targeted speaker's speech characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Streaming Multitalker ASR Model\n",
    "\n",
    "As we covered in the background section, SSA-based multitalker ASR uses external streaming speaker diarization logit values to inform the multitalker model to only concentrate on the targeted speaker. Thus, we only train the ASR model while the speaker diarization model is frozen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the mini Librispeech (English) dataset. It is OK for the purposes of this tutorial, but for anything real, you will need to get at least the entire Librispeech dataset (960 hrs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading MiniLibrispeech\n",
    "!mkdir -p datasets/mini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `get_librispeech_data.py` script located in the nemo/scripts/dataset_processing dir if you cloned NeMo repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"get_librispeech_data.py\"):\n",
    "    !wget https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/dataset_processing/get_librispeech_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python get_librispeech_data.py \\\n",
    "  --data_root \"datasets/mini/\" \\\n",
    "  --data_sets mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prepare the LibriSpeechMix dataset using this MiniLibrispeech dataset. First, adding speaker ids to the manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def add_speaker_ids(manifest_path):\n",
    "    new_data = []\n",
    "    with open(manifest_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            data[\"speaker_id\"] = os.path.basename(data[\"audio_filepath\"]).split(\"-\")[0]     \n",
    "            new_data.append(data)\n",
    "\n",
    "    with open(manifest_path.replace(\".json\", \"_spk.json\"), \"w\") as f:\n",
    "        for data in new_data:\n",
    "            f.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "add_speaker_ids(\"datasets/mini/train_clean_5.json\")\n",
    "add_speaker_ids(\"datasets/mini/dev_clean_2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lhotse import CutSet\n",
    "from nemo.collections.asr.parts.utils.asr_multispeaker_utils import MultiSpeakerMixtureGenerator\n",
    "\n",
    "train_manifest = \"datasets/mini/train_clean_5_spk.json\"\n",
    "val_manifest = \"datasets/mini/dev_clean_2_spk.json\"\n",
    "\n",
    "train_cuts = CutSet(\n",
    "        MultiSpeakerMixtureGenerator(\n",
    "            manifest_filepath=train_manifest,\n",
    "            simulator_type=\"lsmix\",\n",
    "            sample_rate=16000,\n",
    "            min_delay=0.5,\n",
    "            min_duration=0.1,\n",
    "            max_duration=60,\n",
    "            num_speakers=2\n",
    "        )\n",
    "    )\n",
    "\n",
    "train_cuts = CutSet([train_cuts[i] for i in range(10)])\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some samples in the training cuts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = train_cuts[0]\n",
    "\n",
    "print(\"Speaker 0: \")\n",
    "print('Start time: ', cut.supervisions[0].start)\n",
    "print('End time: ', cut.supervisions[0].end)\n",
    "print('Text: ', cut.supervisions[0].text)\n",
    "\n",
    "print(\"Speaker 1: \")\n",
    "print('Start time: ', cut.supervisions[1].start)\n",
    "print('End time: ', cut.supervisions[1].end)\n",
    "print('Text: ', cut.supervisions[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play this audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(cut.load_audio()[0], rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nemo.collections.asr.models import ASRModel\n",
    "import torch\n",
    "\n",
    "config = ASRModel.from_pretrained(\"nvidia/multitalker-parakeet-streaming-0.6b-v1\", return_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.train_ds.input_cfg = \"train_cfg.yaml\"\n",
    "config.validation_ds.input_cfg = \"val_cfg.yaml\"\n",
    "print(config.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "train_cfg = [{\n",
    "    \"input_cfg\": [\n",
    "        {\n",
    "            \"type\": \"multi_speaker_simulator\",\n",
    "            \"manifest_filepath\": train_manifest,\n",
    "            \"weight\": 1,\n",
    "            \"simulator_type\": \"lsmix\", \n",
    "            \"num_speakers\": 2,\n",
    "            \"min_delay\": 0.5,\n",
    "            \"is_tarred\": True\n",
    "        }\n",
    "    ],\n",
    "    \"type\": \"group\"\n",
    "}]\n",
    "with open(\"train_cfg.yaml\", 'w') as f:\n",
    "    f.write(yaml.dump(train_cfg, sort_keys=False))\n",
    "\n",
    "val_cfg = [{\n",
    "    \"input_cfg\": [\n",
    "        {\n",
    "            \"type\": \"multi_speaker_simulator\",\n",
    "            \"manifest_filepath\": val_manifest,\n",
    "            \"weight\": 1,\n",
    "            \"simulator_type\": \"lsmix\", \n",
    "            \"num_speakers\": 2,\n",
    "            \"min_delay\": 0.5,\n",
    "            \"is_tarred\": True\n",
    "        }\n",
    "    ],\n",
    "    \"type\": \"group\"\n",
    "}]\n",
    "with open(\"val_cfg.yaml\", 'w') as f:\n",
    "    f.write(yaml.dump(val_cfg, sort_keys=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload this model with updated config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat train_cfg.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's modify some trainer configs for this demo\n",
    "# Checks if we have GPU available and uses it\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    strategy=\"auto\",\n",
    "    devices=1,\n",
    "    \n",
    "    accelerator=accelerator,\n",
    "    max_epochs=-1,\n",
    "    max_steps=1000,\n",
    "    limit_train_batches=100,\n",
    "    limit_val_batches=5,\n",
    "    enable_progress_bar=True,\n",
    "    check_val_every_n_epoch=1\n",
    ")\n",
    "\n",
    "config.train_ds.batch_size=4\n",
    "config.train_ds.max_duration=20\n",
    "config.validation_ds.batch_size=4\n",
    "config.validation_ds.max_duration=20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "asr_model = ASRModel.from_pretrained(\"nvidia/multitalker-parakeet-streaming-0.6b-v1\", override_config_path=config, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(asr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] [Speaker Targeting via Self-Speaker Adaptation for Multi-talker ASR](https://arxiv.org/abs/2506.22646)  \n",
    "\n",
    "\n",
    "[2] [Stateful Conformer with Cache-based Inference for Streaming Automatic Speech Recognition](https://arxiv.org/abs/2312.17279)  \n",
    "\n",
    "[3] [Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization with Arrival-Time Ordering](https://arxiv.org/abs/2507.18446)\n",
    "\n",
    "[4] [NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks](https://arxiv.org/abs/2408.13106)\n",
    "\n",
    "[5] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://arxiv.org/abs/2305.05084)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ASR_with_NeMo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nemo012826",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
