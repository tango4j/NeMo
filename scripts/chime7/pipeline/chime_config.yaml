gpu_id: 0
num_workers: 16
audio_type: "flac"

chime_data_root: /media/data2/chime7-challenge/datasets/chime7_official_cleaned_v2
output_root: ./nemo_experiments

scenarios:
  - mixer6

subsets: 
  - dev

diar_base_dir: "/media/data2/chime7-challenge/chime7_diar_results"
diar_config: "system_B_V05_D03_bak"
diar_param: "T0.5"


asr_model_path: /media/data2/chime7-challenge/checkpoints/rno_chime7_chime6_ft_ptDataSetasrset3_frontend_nemoGSSv1_prec32_layers24_heads8_conv5_d1024_dlayers2_dsize640_bs128_adamw_CosineAnnealing_lr0.0001_wd1e-2_spunigram1024.nemo
lm_model_path: /media/data2/chime7-challenge/checkpoints/7gram_0001.kenlm


gss_output_dir: ${output_root}/gss_outputs
asr_output_dir: ${output_root}/asr_outputs
eval_output_dir: ${output_root}/eval_outputs


preprocess:
  text_norm: chime7  # choices in [chime6, chime7]
  ignore_shorter: 0.2  # ignore audios shorter than this duration (in seconds)

gss:  
  top_k_channels: 80  # select top_k percentage of best channels from audios [0,100]
  # Args similar to the original GSS implementaion
  # See https://github.com/desh2608/gss for parameter explanations
  bss_iterations: 5
  context_duration: 15
  use_garbage_class: true
  min_segment_length: 0.0
  max_segment_length: 200
  max_batch_duration: 100
  max_batch_cuts: 1
  num_buckets: 4
  force_overwrite: false
  duration_tolerance: 3.0
  channels: null 
  torchaudio_backend: "soundfile"
  dereverb_filter_length: 5
  mc_mask_min_db: -60
  mc_postmask_min_db: -9
  dereverb_prediction_delay: 2
  dereverb_num_iterations: 3
  mc_filter_type: pmwf
  mc_filter_num_iterations: 5
  mc_filter_postfilter: "ban"


asr:
  # Normalize audio to this dB level
  normalize_db: null  

  model_path: ${asr_model_path}  # Path to a .nemo file
  pretrained_name: null  # Name of a pretrained model
  audio_dir: null  # Path to a directory which contains audio files
  dataset_manifest: null  # Path to dataset's JSON manifest or a dir containing manifests
  channel_selector: "average"  # Used to select a single channel from multichannel audio, or use `average` across channels
  audio_key: 'audio_filepath'  # Used to override the default audio key in dataset_manifest
  eval_config_yaml: null  # Path to a yaml file of config of evaluation

  # General configs
  output_filename: null
  batch_size: 8
  num_workers: ${num_workers}
  append_pred: False  # Sets mode of work, if True it will add new field transcriptions.
  pred_name_postfix: null # If you need to use another model name, rather than standard one.
  random_seed: null  # seed number going to be used in seed_everything()

  # Set to True to output greedy timestamp information (only supported models)
  compute_timestamps: False

  # Set to True to output language ID information
  compute_langs: False

  # Set `cuda` to int to define CUDA device. If 'None', will look for CUDA
  # device anyway, and do inference on CPU only if CUDA device is not found.
  # If `cuda` is a negative number, inference will be on CPU only.
  cuda: null
  allow_mps: False  # allow to select MPS device (Apple Silicon M-series GPU)
  amp: False
  audio_type: "wav"  # GSS output wav files

  # Recompute model transcription, even if the output folder exists with scores.
  overwrite_transcripts: True

  # Decoding strategy for CTC models
  ctc_decoding: null

  # Decoding strategy for RNNT models
  rnnt_decoding: 
    strategy: "maes" 
    beam:
      ngram_lm_model: ${lm_model_path}
      ngram_lm_alpha: 0.0
      beam_size: 8 
      return_best_hypothesis: true
      maes_num_steps: 5
      maes_prefix_alpha: 3
      maes_expansion_gamma: 2.8
      maes_expansion_beta: 5

  # decoder type: ctc or rnnt, can be used to switch between CTC and RNNT decoder for Joint RNNT/CTC models
  decoder_type: null

  # Use this for model-specific changes before transcription
  model_change: 
    conformer: 
      # Change self_attention_model for Conformer
      # Options:
      #  'rel_pos': relative positional embedding and Transformer-XL
      #  'rel_pos_local_attn': relative positional embedding and Transformer-XL with local attention using
      #   overlapping chunks. Attention context is determined by att_context_size parameter.
      #  'abs_pos': absolute positional embedding and Transformer
      # If None is provided, self_attention_model is not changed.
      self_attention_model: null

      # Change the attention context size by providing 2 integers,
      # corresponding to left and right context, or -1 for full context.
      # If None is provided, the attention context size isn't changed.
      att_context_size: null

eval:
  dasr_root: ${chime_data_root}
  hyp_folder: null
  partition: dev
  output_folder: null
  # Whether or not use diarization to re-order the system output, if "set to false we will not re-order 
  #the system output (you can set to false if you are using oracle diarization)
  diarization: True 
  falign: false  # Path to folder containing forced-alignment rttms. (not used now)
  collar: 500  # Diarization metrics collar in ms. 500ms collar in pyannote is equivalent to 250ms start and end collar in dscore.
  # If ignore_missing=True, it will ignore missing JSON for a particular scenario, in case you want to score e.g. only DiPCo. 
  # If False, missing the corresponding JSON for a particular scenario will raise an error.
  ignore_missing: True 



