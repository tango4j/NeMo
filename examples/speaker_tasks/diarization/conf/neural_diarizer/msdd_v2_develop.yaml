# Mutiscale diarization decoder (MSDD) is a speaker diarization model based on initializing clustering and multiscale segmentation input.
# Model name convention for MSDD: msdd_<number of scales>scl_<longest scale in decimal second (ds)>_<shortest scale in decimal second (ds)>_<overlap percentage of window shifting>Povl_<hidden layer size>x<number of LSTM layers>x<number of CNN output channels>x<repetition count of conv layer>
# (Example) `msdd_5scl_15_05_50Povl_256x3x32x2.yaml` has 5 scales, the longest scale is 1.5 sec, the shortest scale is 0.5 sec, with 50 percent overlap, hidden layer size is 256, 3 LSTM layers, 32 CNN channels, 2 repeated Conv layers
# MSDD model checkpoint (.ckpt) and NeMo file (.nemo) contain speaker embedding model (TitaNet) and the speaker model is loaded along with standalone MSDD moodule.
# Note that MSDD models require more than one scale. Thus, the parameters in diarizer.speaker_embeddings.parameters should have more than one scale to function as a MSDD model.
# Example: a manifest line for training 
# {"audio_filepath": "/path/to/audio01.wav", "offset": 390.83, "duration": 13.45, "text": "-", "num_speakers": 2, "rttm_filepath": "/path/to/audio01.rttm"}
name: "MultiscaleDiarDecoder" 
sample_rate: 16000
num_workers: 30
batch_size: 8

model: 
  diarizer:
    out_dir: null
    oracle_vad: True # If True, uses RTTM files provided in manifest file to get speech activity (VAD) timestamps
    
    # vad:
    #   model_path: vad_multilingual_marblenet # .nemo local model path or pretrained VAD model name 
    #   parameters: # Tuned parameters for CH109 (using the 11 multi-speaker sessions as dev set) 
    #     window_length_in_sec: 0.15  # Window length in sec for VAD context input 

    speaker_embeddings:
      model_path: ??? # .nemo local model path or pretrained model name (titanet_large is recommended)
      parameters:
        # window_length_in_sec: [1.5,0.75,0.5] # Window length(s) in sec (floating-point number). either a number or a list. ex) 1.5 or [1.5,1.0,0.5]
        # shift_length_in_sec: [0.75,0.375,0.25] # Shift length(s) in sec (floating-point number). either a number or a list. ex) 0.75 or [0.75,0.5,0.25]
        window_length_in_sec: [1.6,0.8,0.4] # Window length(s) in sec (floating-point number). either a number or a list. ex) 1.5 or [1.5,1.0,0.5]
        shift_length_in_sec: [0.8,0.4,0.2] # Shift length(s) in sec (floating-point number). either a number or a list. ex) 0.75 or [0.75,0.5,0.25]
        # window_length_in_sec: [3.0,1.5,0.5] # Window length(s) in sec (floating-point number). either a number or a list. ex) 1.5 or [1.5,1.0,0.5]
        # shift_length_in_sec: [1.5,0.75,0.25] # Shift length(s) in sec (floating-point number). either a number or a list. ex) 0.75 or [0.75,0.5,0.25]
        multiscale_weights: [1,1,1] # Weight for each scale. should be null (for single scale) or a list matched with window/shift scale count. ex) [0.33,0.33,0.33]
        save_embeddings: True # Save embeddings as pickle file for each audio input.

  num_workers: ${num_workers}
  max_num_of_spks: 6 # Number of speakers per model. This is currently fixed at 6.
  scale_n: 3 # Number of scales for MSDD model and initializing clustering.
  interpolated_scale: 0.08 # The length of the interpolated scale
  soft_label_thres: 0.5 # Threshold for creating discretized speaker label from continuous speaker label in RTTM files.
  emb_batch_size: 0 # If this value is bigger than 0, corresponding number of embedding vectors are attached to torch graph and trained.
  session_len_sec: 15
  freeze_speaker_model: True # If True, speaker model is frozen and only MSDD module is trained.
  # freeze_speaker_model: False
  random_flip: False

  train_ds:
    manifest_filepath: ???
    emb_dir: ???
    sample_rate: ${sample_rate}
    num_spks: ${model.max_num_of_spks}
    soft_label_thres: ${model.soft_label_thres}
    session_len_sec: ${model.session_len_sec}
    random_flip: ${model.random_flip}
    labels: null
    batch_size: ${batch_size}
    emb_batch_size: ${model.emb_batch_size}
    shuffle: True

  validation_ds:
    manifest_filepath: ???
    emb_dir: ???
    sample_rate: ${sample_rate}
    num_spks: ${model.max_num_of_spks}
    soft_label_thres: ${model.soft_label_thres}
    session_len_sec: ${model.session_len_sec}
    random_flip: False
    
    labels: null
    batch_size: ${batch_size}
    emb_batch_size: ${model.emb_batch_size}
    shuffle: False
  
  test_ds:
    manifest_filepath: null
    emb_dir: null
    sample_rate: 16000
    num_spks: ${model.max_num_of_spks}
    soft_label_thres: ${model.soft_label_thres}
    session_len_sec: ${model.session_len_sec}
    random_flip: False
    labels: null
    batch_size: ${batch_size}
    shuffle: False
    seq_eval_mode: True

  preprocessor:
    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor
    normalize: "per_feature"
    window_size: 0.025
    sample_rate: ${sample_rate}
    window_stride: 0.01
    window: "hann"
    features: 80
    n_fft: 512
    frame_splicing: 1
    dither: 0.00001

  msdd_module:
    _target_: nemo.collections.asr.modules.msdd_diarizer.MSDD_module
    num_spks: ${model.max_num_of_spks} # Number of speakers per model. This is currently fixed at 2.
    hidden_size: 256 # Hidden layer size for linear layers in MSDD module
    num_lstm_layers: 3 # Number of stacked LSTM layers
    dropout_rate: 0.5 # Dropout rate
    cnn_output_ch: 32 # Number of filters in a conv-net layer.
    conv_repeat: 2 # Determins the number of conv-net layers. Should be greater or equal to 1.
    emb_dim: 192 # Dimension of the speaker embedding vectors
    scale_n: ${model.scale_n} # Number of scales for multiscale segmentation input
    weighting_scheme: 'attn_scale_weight' # Type of weighting algorithm. Options: ('conv_scale_weight', 'attn_scale_weight')
    context_vector_type: 'cos_sim_vad' # Type of context vector: options. Options: ('cos_sim', 'elem_prod')

  msdd_classifier:
    _target_: nemo.collections.asr.modules.transformer.transformer_encoders.TransformerEncoder
    # _target_: nemo.collections.asr.modules.transformer.transformer_decoders.TransformerDecoder
    num_layers: 4
    hidden_size: 256 # Needs to be multiple of num_attention_heads
    inner_size: 384
    num_attention_heads: 4
    attn_score_dropout: 0.5
    attn_layer_dropout: 0.5
    ffn_dropout: 0.5
    hidden_act: relu
    pre_ln: False
    pre_ln_final_layer_norm: True

  loss: 
    _target_: nemo.collections.asr.losses.bce_loss.BCELoss
    weight: null # Weight for binary cross-entropy loss. Either `null` or list type input. (e.g. [0.5,0.5])
    alpha: 0.003 # Ratio between BCE loss and affinity loss
  
  optim_param_groups:
    msdd._speaker_model.encoder: # NOTE: Only EncDecDiarLabelModel class supports multi-level optim_param_groups
      lr: .00001
      # lr: .0
      weight_decay: 0.005

      sched:
        name: CosineAnnealing
        min_lr: 0.0

  optim:
    name: adam
    lr: .0001
    weight_decay: 0.005

    sched:
      name: CosineAnnealing
      # min_lr: 0.00001
      min_lr: 0.0000001

trainer:
  devices: 1 # number of gpus (devices)
  accelerator: gpu 
  max_epochs: 5000
  max_steps: -1 # computed at runtime if not set
  num_nodes: 1
  strategy: ddp
  resume_from_checkpoint: null
  accumulate_grad_batches: 1
  deterministic: True
  enable_checkpointing: False
  logger: False
  log_every_n_steps: 1  # Interval of logging.
  val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations

exp_manager:
  exp_dir: null
  name: ${name}
  create_tensorboard_logger: True
  create_checkpoint_callback: True
  create_wandb_logger: False
  checkpoint_callback_params:
    monitor: "val_loss"
    mode: "min"
    save_top_k: 30
    every_n_epochs: 1
  wandb_logger_kwargs:
    name: null
    project: null
