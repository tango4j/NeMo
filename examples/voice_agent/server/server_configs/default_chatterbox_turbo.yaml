# Example server config for using ChatterBox Turbo TTS
# This is an example config for setting up a NeMo Voice Agent server with ChatterBox Turbo TTS.
# Please refer to https://github.com/NVIDIA-NeMo/NeMo/tree/main/examples/voice_agent/README.md for more details

transport:
  audio_out_10ms_chunks: 10  # use 4 as websocket default, but increasing to a larger number might have less glitches in TTS output

vad:
  type: silero
  confidence: 0.6  # VAD threshold for detecting speech versus non-speech
  start_secs: 0.1  # min amount of speech to trigger UserStartSpeaking
  stop_secs: 1.2  # min amount of silence to trigger UserStopSpeaking
  min_volume: 0.4  # Microphone volumn threshold for VAD

stt:
  type: nemo # choices in ['nemo'] currently only NeMo is supported
  model: "nvidia/parakeet_realtime_eou_120m-v1"
  model_config: "./server_configs/stt_configs/nemo_cache_aware_streaming.yaml"
  device: "cuda"

diar:
  type: nemo
  enabled: true # set to false to disable
  model: "nvidia/diar_streaming_sortformer_4spk-v2.1"
  device: "cuda"
  threshold: 0.4  # threshold value used to determine if a speaker exists or not, setting it to a lower value will increaset the sensitivity of the model
  frame_len_in_secs: 0.08  # default for Sortformer, do not change unless using other architechtures

turn_taking:
  backchannel_phrases_path: "./server/backchannel_phrases.yaml"  # set it to the actual path of the file, or specify a list of backchannel phrases here
  max_buffer_size: 2  # num of words more than this amount will interrupt the LLM immediately if not backchannel phrases
  bot_stop_delay: 0.5  # a delay in seconds allowed between server and client audio output, so that the BotStopSpeaking signal is handled not too far away from the actual time that the user hears all audio output

llm:
  type: auto  # choices in ['auto', 'hf', 'vllm'], if `auto`, it will try to use vllm and fall back to hf if vllm not available
  model: "nvidia/NVIDIA-Nemotron-Nano-9B-v2"  # model name for HF models, will be used via `AutoModelForCausalLM.from_pretrained()`
  model_config: "./server_configs/llm_configs/nemotron_nano_v2.yaml"
  device: "cuda"
  enable_reasoning: false  # it's best to turn-off reasoning for lowest latency, setting it to True will use the same config ending with `_think.yaml` instead
  system_prompt: "You are a helpful AI agent named Lisa. Start by greeting the user warmly and introducing yourself within one sentence. Your answer should be concise and to the point. You might also see speaker tags (<speaker_0>, <speaker_1>, etc.) in the user context. You should respond to the user based on the speaker tag and the context of that speaker. Do not include the speaker tags in your response, use them only to identify the speaker. Avoid using emoji in your response."

tts:
  type: chatterboxturbo  # Use ChatterBox Turbo TTS
  model: "ResembleAI/chatterbox-turbo"  # Model ID in registry
  model_config: "./server_configs/tts_configs/chatterbox_turbo.yaml"  # Optional: path to config file
  device: "cuda"
  
  # ChatterBox Turbo specific notes:
  # - The model will be downloaded from HuggingFace on first use
  # - Sample rate is fixed at 44100 Hz
  # - For voice cloning, set sub_model_id to path of audio file (>5 seconds) in chatterbox_turbo.yaml
  # - Speed can be adjusted from 0.1 to 2.0 (default: 1.0)
