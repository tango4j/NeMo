inference:
  greedy: false # Whether or not to use sampling ; use greedy decoding otherwise
  top_k: 0 # The number of highest probability vocabulary tokens to keep for top-k-filtering.
  top_p: 0.9 # If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.
  temperature: 1.0 # sampling temperature
  add_BOS: true # add the bos token at the begining of the prompt
  tokens_to_generate: 30 # The minimum length of the sequence to be generated.
  all_probs: false # whether return the log prob for all the tokens in vocab
  repetition_penalty: 1.2 # The parameter for repetition penalty. 1.0 means no penalty.
  min_tokens_to_generate: 0 # The minimum length of the sequence to be generated.
  compute_logprob: false # a flag used to compute logprob of all the input text, a very special case of running inference, default False
  batch_size: 1 # batch size for inference
  max_context_length: 512 # max length of the context, input sequence will be truncated if it is longer than this

trainer:
  devices: 1
  num_nodes: 1
  accelerator: gpu
  logger: false # logger provided by exp_manager
  precision: bf16 # 16, 32, or bf16
  enable_checkpointing: false

model:
  tensor_model_parallel_size: 1 # Pruning currently only supports tensor_model_parallel_size=1
  pipeline_model_parallel_size: 1
  sequence_parallel: false # Sequence parallelism is not supported with pipeline parallelism
  restore_from_path: ??? # Nemo file path

  ## Activation Checkpoint
  activations_checkpoint_granularity: null # 'selective' or 'full'
  activations_checkpoint_method: null # 'uniform', 'block', not used with 'selective'

prune:
  calib_dataset: wikitext # wikitext, cnn_dailymail, or a local dataset
  num_calib_size: 1024 # number of samples used for calibration
  # pruning constraints (null means no pruning)
  ffn_hidden_size: null # ffn_hidden_size in the pruned model
  num_attention_heads: null # num_attention_heads in the pruned model
  num_query_groups: null # num_query_groups in the pruned model
  hidden_size: null # hidden_size (embedding size) in the pruned model
  num_layers: null # num_layers (depth) in the pruned model using on cosine-similarity based importance
  drop_layers: [] # drop specified layer numbers (comma separated, 1-indexed). Cannot be used with other constraints

export:
  save_path: ??? # Path where the pruned model will be saved
